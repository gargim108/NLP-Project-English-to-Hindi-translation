{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import fasttext\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset run (/home/cgiuser/.cache/huggingface/datasets/run/raw_datasets/0.0.0/2cae6c9928007a851cc344dbeef11917d3313fc7890d64b041624cabd5a73db3)\n"
     ]
    }
   ],
   "source": [
    "# download the dataset using run,py\n",
    "# use datasets library for downloading\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"./run.py\",  \"raw_datasets\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'source': 'ted',\n",
       " 'alignment_type': '1-1',\n",
       " 'alignment_quality': '1.0',\n",
       " 'translation': {'en': \"I'd like to tell you about one such child,\",\n",
       "  'hi': 'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw view of data in dataset\n",
    "\n",
    "raw_datasets[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273885/273885 [00:13<00:00, 20936.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe using raw data in dataset.\n",
    "\n",
    "final_lst = []\n",
    "\n",
    "for trans in tqdm(raw_datasets):\n",
    "    eng = trans['translation']['en']\n",
    "    hi = trans['translation']['hi']\n",
    "    final_lst.append([eng,hi])\n",
    "    \n",
    "trans_data = pd.DataFrame(final_lst,columns=['eng_sent','hi_sent'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_sent</th>\n",
       "      <th>hi_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140505</th>\n",
       "      <td>people we're analyzing are based?</td>\n",
       "      <td>हम का विश्लेषण कर रहे हैं लोगों के लिए कर रहे ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118346</th>\n",
       "      <td>And I was astounded to hear the little fellow ...</td>\n",
       "      <td>इस बच्चे का जवाब सुनकर तो मैं अवाक् रह गया -</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 eng_sent  \\\n",
       "140505                  people we're analyzing are based?   \n",
       "118346  And I was astounded to hear the little fellow ...   \n",
       "\n",
       "                                                  hi_sent  \n",
       "140505  हम का विश्लेषण कर रहे हैं लोगों के लिए कर रहे ...  \n",
       "118346       इस बच्चे का जवाब सुनकर तो मैं अवाक् रह गया -  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glimpse of dataframe\n",
    "\n",
    "trans_data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  politicians do not have permission to do what needs to be done.\n",
      "****************************************************************************************************\n",
      "Hindi sentence:  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .\n"
     ]
    }
   ],
   "source": [
    "# glimpse of translation\n",
    "\n",
    "print('English sentence: ',trans_data['eng_sent'].iloc[1])\n",
    "print('*'*100)\n",
    "print('Hindi sentence: ',trans_data['hi_sent'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-26 04:37:34--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.164.202.10, 18.164.202.35, 18.164.202.120, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.164.202.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 938013 (916K) [binary/octet-stream]\n",
      "Saving to: ‘lid.176.ftz.5’\n",
      "\n",
      "lid.176.ftz.5       100%[===================>] 916.03K  4.91MB/s    in 0.2s    \n",
      "\n",
      "2023-10-26 04:37:34 (4.91 MB/s) - ‘lid.176.ftz.5’ saved [938013/938013]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can use fasttext to identify the language\n",
    "# https://fasttext.cc/docs/en/language-identification.html\n",
    "# download the model from given path\n",
    "\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__hi',), array([0.97123677]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# import fasttext and predict using model loaded.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = fasttext.load_model('lid.176.ftz')\n",
    "print(model.predict('राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है', k=1))  # top 2 matching languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng_sent    0\n",
       "hi_sent     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any null values\n",
    "\n",
    "trans_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Observation : There are no null values in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lowercase all characters\n",
    "trans_data['eng_sent']=trans_data['eng_sent'].apply(lambda x: x.lower())\n",
    "\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"won\\'t\", \"will not\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"can\\'t\", \"can not\", x))\n",
    "\n",
    "# general\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"n\\'t\", \" not\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"\\'re\", \" are\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"\\'s\", \" is\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"\\'d\", \" would\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"\\'ll\", \" will\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"\\'t\", \" not\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"\\'ve\", \" have\", x))\n",
    "trans_data['eng_sent'] = trans_data['eng_sent'].apply(lambda x: re.sub(r\"\\'m\", \" am\", x))\n",
    "\n",
    "# Remove quotes\n",
    "trans_data['eng_sent']=trans_data['eng_sent'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "trans_data['hi_sent']=trans_data['hi_sent'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "# Remove all the special characters\n",
    "trans_data['eng_sent']=trans_data['eng_sent'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "trans_data['hi_sent']=trans_data['hi_sent'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "trans_data['eng_sent']=trans_data['eng_sent'].apply(lambda x: x.translate(remove_digits))\n",
    "trans_data['hi_sent']=trans_data['hi_sent'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "trans_data['hi_sent'] = trans_data['hi_sent'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "# if eng_sent text in hi_sent column\n",
    "trans_data['hi_sent'] = trans_data['hi_sent'].apply(lambda x: re.sub(\"[A-Za-z]\", \"\", x))\n",
    "\n",
    "# remove extra\n",
    "trans_data['eng_sent']=trans_data['eng_sent'].apply(lambda x: re.sub('[-_.:;\\[\\]\\|,]', '', x))\n",
    "trans_data['hi_sent']=trans_data['hi_sent'].apply(lambda x: re.sub('[-_.;\\[\\]\\|,]', '', x))\n",
    "\n",
    "# Remove extra spaces\n",
    "trans_data['eng_sent']=trans_data['eng_sent'].apply(lambda x: x.strip())\n",
    "trans_data['hi_sent']=trans_data['hi_sent'].apply(lambda x: x.strip())\n",
    "trans_data['eng_sent']=trans_data['eng_sent'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "trans_data['hi_sent']=trans_data['hi_sent'].apply(lambda x: re.sub(\" +\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  i would like to tell you about one such child\n",
      "****************************************************************************************************\n",
      "Hindi sentence:  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी\n"
     ]
    }
   ],
   "source": [
    "# glimpse of translation\n",
    "\n",
    "print('English sentence: ',trans_data['eng_sent'].iloc[2])\n",
    "print('*'*100)\n",
    "print('Hindi sentence: ',trans_data['hi_sent'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_sent</th>\n",
       "      <th>hi_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sharaabi</td>\n",
       "      <td>शराबी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng_sent  \\\n",
       "0                                           sharaabi   \n",
       "1  politicians do not have permission to do what ...   \n",
       "\n",
       "                                             hi_sent  \n",
       "0                                              शराबी  \n",
       "1  राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check language of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273885it [01:07, 4072.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# we predict language type using fasttext\n",
    "# then match with given lang\n",
    "# if not matchinhg then we remove those rows\n",
    "\n",
    "trans_data['is_hi_sent'] = True\n",
    "trans_data['is_eng_sent'] = True\n",
    "\n",
    "error = []\n",
    "count = 0\n",
    "for i,row in tqdm(trans_data.iterrows()):\n",
    "    hi_tex = trans_data.iloc[i]['hi_sent']\n",
    "    en_tex = trans_data.iloc[i]['eng_sent']\n",
    "    try:\n",
    "        hin_pred = model.predict(hi_tex,k=3)[0]\n",
    "        if set(['__label__hi']).issubset(hin_pred) or set(['__label__mr']).issubset(hin_pred):\n",
    "            pass\n",
    "        else:\n",
    "            trans_data.at[i,'is_hi_sent'] = False\n",
    "            count += 1\n",
    "        en_pred = model.predict(en_tex,k=3)[0]\n",
    "        if set(['__label__en']).issubset(en_pred):\n",
    "            pass\n",
    "        else:\n",
    "            trans_data.at[i,'is_eng_sent'] = False\n",
    "            count += 1\n",
    "    except:\n",
    "        error.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where sent is not in english :  7998\n",
      "****************************************************************************************************\n",
      "Where sent is not in hindi :  5132\n"
     ]
    }
   ],
   "source": [
    "print('Where sent is not in english : ',trans_data[trans_data.is_eng_sent == False].shape[0])\n",
    "print(\"*\"*100)\n",
    "print('Where sent is not in hindi : ',trans_data[trans_data.is_hi_sent == False].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing not matching data : 261117\n"
     ]
    }
   ],
   "source": [
    "# remove the data where it is not matching\n",
    "\n",
    "trans_data = trans_data[~trans_data.is_hi_sent == False]\n",
    "trans_data = trans_data[~trans_data.is_eng_sent == False]\n",
    "\n",
    "\n",
    "print('After removing not matching data :',trans_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "\n",
    "trans_data.drop(columns=['is_hi_sent','is_eng_sent'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trans_data['hindi_len'] = trans_data['hi_sent'].apply(lambda x: len(x.split()))\n",
    "trans_data['english_len'] = trans_data['eng_sent'].apply(lambda x: len(x.split()))\n",
    "\n",
    "sns.histplot(trans_data.hindi_len, bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(trans_data.english_len, bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Quantile is 1.0\n",
      "20 Quantile is 2.0\n",
      "30 Quantile is 3.0\n",
      "40 Quantile is 5.0\n",
      "50 Quantile is 8.0\n",
      "60 Quantile is 11.0\n",
      "70 Quantile is 15.0\n",
      "80 Quantile is 20.0\n",
      "90 Quantile is 30.0\n",
      "100 Quantile is 416.0\n"
     ]
    }
   ],
   "source": [
    "# we can identify the length of text in percentile\n",
    "for i in np.arange(0.1,1.1,0.1):\n",
    "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(trans_data.hindi_len, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 Quantile is 30.0\n",
      "91 Quantile is 31.0\n",
      "92 Quantile is 33.0\n",
      "93 Quantile is 35.0\n",
      "94 Quantile is 38.0\n",
      "95 Quantile is 41.0\n",
      "96 Quantile is 45.0\n",
      "97 Quantile is 51.0\n",
      "98 Quantile is 61.0\n",
      "99 Quantile is 82.0\n",
      "100 Quantile is 416.0\n"
     ]
    }
   ],
   "source": [
    "# hindi\n",
    "for i in np.arange(0.9,1.01,0.01):\n",
    "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(trans_data.hindi_len, i)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "95 percentile values for len of hindi statement is around 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Quantile is 1.0\n",
      "20 Quantile is 2.0\n",
      "30 Quantile is 3.0\n",
      "40 Quantile is 5.0\n",
      "50 Quantile is 7.0\n",
      "60 Quantile is 10.0\n",
      "70 Quantile is 13.0\n",
      "80 Quantile is 18.0\n",
      "90 Quantile is 27.0\n",
      "100 Quantile is 629.0\n"
     ]
    }
   ],
   "source": [
    "# we can identify the length of text in percentile\n",
    "for i in np.arange(0.1,1.1,0.1):\n",
    "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(trans_data.english_len, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 Quantile is 27.0\n",
      "91 Quantile is 29.0\n",
      "92 Quantile is 31.0\n",
      "93 Quantile is 32.0\n",
      "94 Quantile is 35.0\n",
      "95 Quantile is 38.0\n",
      "96 Quantile is 42.0\n",
      "97 Quantile is 49.0\n",
      "98 Quantile is 59.0\n",
      "99 Quantile is 81.84000000002561\n",
      "100 Quantile is 629.0\n"
     ]
    }
   ],
   "source": [
    "# englist sent\n",
    "for i in np.arange(0.9,1.01,0.01):\n",
    "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(trans_data.english_len, i)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "95 percentile values for len of english statement is around 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final rows of data :  245937\n"
     ]
    }
   ],
   "source": [
    "# we can limit len of sentence to 40\n",
    "\n",
    "trans_data = trans_data[trans_data.english_len<=40]\n",
    "trans_data = trans_data[trans_data.hindi_len<=40]\n",
    "\n",
    "print('Final rows of data : ',trans_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create decoder input and decoder output\n",
    "# for decoder input we have to add start string in start of sent\n",
    "# for decoder output we have to add end string in end of sent\n",
    "\n",
    "trans_data['hi_inp'] = '<start> ' + trans_data.hi_sent \n",
    "trans_data['hi_out'] = trans_data.hi_sent + ' <end>' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_sent</th>\n",
       "      <th>hi_inp</th>\n",
       "      <th>hi_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sharaabi</td>\n",
       "      <td>&lt;start&gt; शराबी</td>\n",
       "      <td>शराबी &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>&lt;start&gt; राजनीतिज्ञों के पास जो कार्य करना चाहि...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng_sent  \\\n",
       "0                                           sharaabi   \n",
       "1  politicians do not have permission to do what ...   \n",
       "\n",
       "                                              hi_inp  \\\n",
       "0                                      <start> शराबी   \n",
       "1  <start> राजनीतिज्ञों के पास जो कार्य करना चाहि...   \n",
       "\n",
       "                                              hi_out  \n",
       "0                                        शराबी <end>  \n",
       "1  राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "\n",
    "trans_data.drop(columns=['english_len','hindi_len', 'hi_sent'],inplace=True)\n",
    "\n",
    "trans_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_data.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>eng_sent</th>\n",
       "      <th>hi_inp</th>\n",
       "      <th>hi_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sharaabi</td>\n",
       "      <td>&lt;start&gt; शराबी</td>\n",
       "      <td>शराबी &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>&lt;start&gt; राजनीतिज्ञों के पास जो कार्य करना चाहि...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           eng_sent  \\\n",
       "0           0                                           sharaabi   \n",
       "1           1  politicians do not have permission to do what ...   \n",
       "\n",
       "                                              hi_inp  \\\n",
       "0                                      <start> शराबी   \n",
       "1  <start> राजनीतिज्ञों के पास जो कार्य करना चाहि...   \n",
       "\n",
       "                                              hi_out  \n",
       "0                                        शराबी <end>  \n",
       "1  राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, validation = train_test_split(trans_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train data :  196749\n",
      "****************************************************************************************************\n",
      "Number of rows in validation data :  49188\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in train data : ',train.shape[0])\n",
    "print(\"*\"*100)\n",
    "print('Number of rows in validation data : ',validation.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <end> in first row so we can use same tokenizer for both hi_inp and eng_out\n",
    "train.hi_inp.iloc[0]= str(train.hi_inp.iloc[0]) + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization on data\n",
    "token_eng = Tokenizer()\n",
    "token_eng.fit_on_texts(train['eng_sent'].values)\n",
    "token_hi = Tokenizer(filters='')\n",
    "token_hi.fit_on_texts(train['hi_inp'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of english is 76357\n",
      "****************************************************************************************************\n",
      "Vocab size of hindi is 87591\n"
     ]
    }
   ],
   "source": [
    "vocab_size_eng=len(token_eng.word_index.keys())\n",
    "print('Vocab size of english is',vocab_size_eng)\n",
    "print(\"*\"*100)\n",
    "vocab_size_hi=len(token_hi.word_index.keys())\n",
    "print('Vocab size of hindi is',vocab_size_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 48906)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_hi.word_index['<start>'], token_hi.word_index['<end>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to numbers\n",
    "\n",
    "max_len = 40\n",
    "# test to sequence\n",
    "train_dec_in = token_hi.texts_to_sequences(train.hi_inp)\n",
    "train_dec_out = token_hi.texts_to_sequences(train.hi_out)\n",
    "train_eng_inp = token_eng.texts_to_sequences(train.eng_sent)\n",
    "\n",
    "# padding\n",
    "train_dec_in_seq = pad_sequences(train_dec_in, maxlen=max_len, padding='post', dtype='int32')\n",
    "train_dec_out_seq = pad_sequences(train_dec_out, maxlen=max_len, padding='post', dtype='int32')\n",
    "train_eng_inp_seq = pad_sequences(train_eng_inp, maxlen=max_len, padding='post', dtype='int32')\n",
    "\n",
    "# test to sequence\n",
    "test_dec_in = token_hi.texts_to_sequences(validation.hi_inp)\n",
    "test_dec_out = token_hi.texts_to_sequences(validation.hi_out)\n",
    "test_eng_inp = token_eng.texts_to_sequences(validation.eng_sent)\n",
    "\n",
    "# padding\n",
    "test_dec_in_seq = pad_sequences(test_dec_in, maxlen=max_len, padding='post', dtype='int32')\n",
    "test_dec_out_seq = pad_sequences(test_dec_out, maxlen=max_len, padding='post', dtype='int32')\n",
    "test_eng_inp_seq = pad_sequences(test_eng_inp, maxlen=max_len, padding='post', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder text : templatebharat ratna\n",
      "Representation : [27575  6846     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "hi text : <start> साँचाभारत रत्न सम्मानित\n",
      "Decoder input : [    1 26268  2992  2210     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "Decoder output : साँचाभारत रत्न सम्मानित <end>\n",
      "Decoder output : [26268  2992  2210 48906     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# check how vectorization has been done\n",
    "\n",
    "index = 4\n",
    "print('Encoder text :',train.eng_sent.iloc[index])\n",
    "print('Representation :',train_eng_inp_seq[index])\n",
    "\n",
    "print('hi text :', train.hi_inp.iloc[index])\n",
    "print('Decoder input :',train_dec_in_seq[index])\n",
    "\n",
    "print('Decoder output :',train.hi_out.iloc[index])\n",
    "print('Decoder output :',train_dec_out_seq[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196749"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_eng_inp_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionaries from num to word and vice versa\n",
    "hi_index_word = {}\n",
    "hi_word_index = {}\n",
    "\n",
    "for key, value in token_hi.word_index.items():\n",
    "    hi_index_word[value] = key\n",
    "    hi_word_index[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_vocab_size = len(token_hi.word_index)+1\n",
    "eng_vocab_size = len(token_eng.word_index)+1\n",
    "\n",
    "enc_input_length, dec_input_length, dec_out_length = 40,40,40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Encoder - Decoder Seq-Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference : https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "\n",
    "def data_generator(encoder_inp, decoder_inp, decoder_out, batch_size):\n",
    "    '''\n",
    "    Data Generator for model training\n",
    "    It returns list of encoder_imput and decoder_input of each shape [batch_size, max_len]\n",
    "    and decoder_out of shape (batch_size, max_len)\n",
    "    '''\n",
    "    while True:\n",
    "        for i in range(0, len(encoder_inp), batch_size):\n",
    "            # creating empty matrix\n",
    "            enc_inp_batch = np.zeros(shape = (batch_size, encoder_inp.shape[-1])) # shape = (batch_size, max_len)\n",
    "            dec_inp_batch = np.zeros(shape = (batch_size, decoder_inp.shape[-1])) # shape = (batch_size, max_len)\n",
    "            dec_out_batch = np.zeros(shape = (batch_size, decoder_out.shape[-1])) # shape = (batch_size, max_len)\n",
    "            for j in range(batch_size):\n",
    "                if (i+j) < len(encoder_inp):\n",
    "                    # adding batch wise values\n",
    "                    enc_inp_batch[j] = encoder_inp[i+j]\n",
    "                    dec_inp_batch[j] = decoder_inp[i+j]\n",
    "                    dec_out_batch[j] = decoder_out[i+j]\n",
    "            # Yield is a keyword in Python that is used to return from a function without \n",
    "            # destroying the states of its local variable and when the function is called, \n",
    "            # the execution starts from the last yield statement.\n",
    "            yield [enc_inp_batch, dec_inp_batch], dec_out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Input, Dense, RNN, LSTMCell, Activation, add, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        self.embedding = Embedding(inp_vocab_size, embedding_size, input_length = input_length)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.lstm_size = lstm_size\n",
    "        lstmcell = LSTMCell(lstm_size)\n",
    "        self.lstm = RNN(lstmcell, return_sequences = True, return_state = True)\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          returns -- encoder_output, last time step's hidden and cell state\n",
    "        '''\n",
    "        embeddings = self.embedding(input_sequence)\n",
    "        \n",
    "        encoder_output, encoder_final_state_h, encoder_final_state_c = self.lstm(embeddings, initial_state = states)\n",
    "        \n",
    "        return encoder_output, encoder_final_state_h, encoder_final_state_c\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      '''\n",
    "      return tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.embed_layer = Embedding(input_dim=out_vocab_size, output_dim=embedding_size, input_length=input_length)\n",
    "        #Intialize Decoder LSTM layer\n",
    "        lstmcell = LSTMCell(lstm_size)\n",
    "        self.lstm_layer = RNN(lstmcell, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,initial_states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
    "        \n",
    "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
    "        '''\n",
    "        x = self.embed_layer(input_sequence)\n",
    "        decoder_output,decoder_final_state_h,decoder_final_state_c = self.lstm_layer(x, initial_state = initial_states)\n",
    "        return decoder_output,decoder_final_state_h,decoder_final_state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, ita_vocab_size, eng_vocab_size, enc_embedding_size, dec_embedding_size, lstm_size,\n",
    "                 enc_input_length, dec_input_length):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encode_obj = Encoder(ita_vocab_size,enc_embedding_size,lstm_size,enc_input_length)\n",
    "        #Create decoder object\n",
    "        self.decode_obj = Decoder(eng_vocab_size,dec_embedding_size,lstm_size,dec_input_length)\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(eng_vocab_size, activation='softmax')\n",
    "        self.enc_input_length = enc_input_length\n",
    "    \n",
    "    def call(self,data):\n",
    "        '''\n",
    "        A. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
    "        B. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
    "        C. Pass the decoder_outputs into Dense layer \n",
    "        \n",
    "        Return decoder_outputs\n",
    "        '''\n",
    "        enc_data = data[0]\n",
    "        dec_data = data[1]\n",
    "\n",
    "        # A\n",
    "        enc_initial_state = self.encode_obj.initialize_states(tf.shape(enc_data)[0])\n",
    "        enc_out, enc_h, enc_c = self.encode_obj(enc_data,enc_initial_state)\n",
    "\n",
    "        # B\n",
    "        dec_out, dec_h, dec_c = self.decode_obj(dec_data, [enc_h, enc_c])\n",
    "        # C\n",
    "        x = self.dense(dec_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_embedding_size = 64\n",
    "dec_embedding_size = 64 \n",
    "lstm_size = 128\n",
    "\n",
    "# defining model\n",
    "en_hi_model = Encoder_decoder(eng_vocab_size, hi_vocab_size, enc_embedding_size, dec_embedding_size, lstm_size,\n",
    "                 enc_input_length, dec_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compile\n",
    "en_hi_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "# call back of tensorboard\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callback = [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 04:39:06.813330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4601/7026 [==================>...........] - ETA: 6:12 - loss: 1.9458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7026/7026 [==============================] - 1401s 199ms/step - loss: 1.5289 - val_loss: 1.4229\n",
      "Epoch 3/10\n",
      "7026/7026 [==============================] - 1401s 199ms/step - loss: 1.3616 - val_loss: 1.3300\n",
      "Epoch 4/10\n",
      "7026/7026 [==============================] - 1400s 199ms/step - loss: 1.2328 - val_loss: 1.2728\n",
      "Epoch 5/10\n",
      "7026/7026 [==============================] - 1404s 200ms/step - loss: 1.1293 - val_loss: 1.2364\n",
      "Epoch 6/10\n",
      "7026/7026 [==============================] - 1403s 200ms/step - loss: 1.0448 - val_loss: 1.2152\n",
      "Epoch 7/10\n",
      "7026/7026 [==============================] - 1402s 200ms/step - loss: 0.9748 - val_loss: 1.2019\n",
      "Epoch 8/10\n",
      "3325/7026 [=============>................] - ETA: 9:28 - loss: 0.9269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030/7026 [===========>..................] - ETA: 10:13 - loss: 0.8294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7026/7026 [==============================] - 1398s 199ms/step - loss: 0.8244 - val_loss: 1.1960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc88deb9f10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 108\n",
    "# send data to data generators\n",
    "train_data_generator = data_generator(train_eng_inp_seq, train_dec_in_seq, train_dec_out_seq, batch_size)\n",
    "val_data_generator = data_generator(test_eng_inp_seq, test_dec_in_seq, test_dec_out_seq, batch_size)\n",
    "\n",
    "# train model\n",
    "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
    "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
    "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7026/7026 [==============================] - 1403s 200ms/step - loss: 0.7885 - val_loss: 1.2001\n",
      "Epoch 2/10\n",
      "7026/7026 [==============================] - 1401s 199ms/step - loss: 0.7576 - val_loss: 1.2055\n",
      "Epoch 3/10\n",
      "7026/7026 [==============================] - 1401s 199ms/step - loss: 0.7308 - val_loss: 1.2118\n",
      "Epoch 4/10\n",
      "7026/7026 [==============================] - 1402s 200ms/step - loss: 0.7074 - val_loss: 1.2192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc86fa17690>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
    "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
    "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1821/1821 [==============================] - 362s 199ms/step - loss: 0.6976 - val_loss: 1.2254\n",
      "Epoch 2/10\n",
      "1821/1821 [==============================] - 362s 199ms/step - loss: 0.7178 - val_loss: 1.2291\n",
      "Epoch 3/10\n",
      "1821/1821 [==============================] - 362s 199ms/step - loss: 0.7195 - val_loss: 1.2244\n",
      "Epoch 4/10\n",
      "1821/1821 [==============================] - 362s 199ms/step - loss: 0.7080 - val_loss: 1.2373\n",
      "Epoch 5/10\n",
      "1177/1821 [==================>...........] - ETA: 1:38 - loss: 0.6643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808/1821 [============================>.] - ETA: 1s - loss: 0.6879"
     ]
    }
   ],
   "source": [
    "batch_size = 108\n",
    "\n",
    "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
    "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
    "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save best model for further use\n",
    "\n",
    "# en_hi_model.save_weights('model_train_26_10.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model  = en_hi_model.load_weights('model_train_latest.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: query resolution aaic team \n",
    "def predict(input_sentence):\n",
    "    \n",
    "    '''\n",
    "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to decoder\n",
    "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "            predicted_out,state_h,state_c=model.layers[1](dec_input,states)\n",
    "            pass the predicted_out to the dense layer\n",
    "            update the states=[state_h,state_c]\n",
    "            And get the index of the word with maximum probability of the dense layer output, using the tokenizer(word index) get the word and then store it in a string.\n",
    "            Update the input_to_decoder with current predictions\n",
    "    F. Return the predicted sentence\n",
    "    '''\n",
    "    # A\n",
    "    # lets tokenize the sentence first\n",
    "    tokenized_encoder_input = token_eng.texts_to_sequences([input_sentence])\n",
    "    # padding the sequence\n",
    "    encoder_input = pad_sequences(tokenized_encoder_input, maxlen=max_len, padding='post', dtype='int32')\n",
    "    \n",
    "    # B\n",
    "    # get the initial encoder states\n",
    "    enc_init_states = en_hi_model.layers[0].initialize_states(1)\n",
    "    enc_out, enc_h_state, enc_c_state = en_hi_model.layers[0](encoder_input, states = enc_init_states)\n",
    "\n",
    "    # C\n",
    "    decoder_initial_states = [enc_h_state, enc_c_state]\n",
    "    decoder_initial_input = np.zeros((1,1))\n",
    "    decoder_initial_input[0,0] = hi_word_index['<start>']\n",
    "\n",
    "    # D\n",
    "    predicted_words = []\n",
    "    predicting = True\n",
    "    while predicting:\n",
    "\n",
    "        dec_out, dec_h_state, dec_c_state = en_hi_model.layers[1](decoder_initial_input, decoder_initial_states)\n",
    "        english_predicted_int = np.argmax(en_hi_model.layers[2](dec_out).numpy().ravel())\n",
    "        predicted_words.append(english_predicted_int)\n",
    "        # replacing the next input to decoder with current decoder output\n",
    "        decoder_initial_input[0,0] = english_predicted_int\n",
    "        # replacing next decoder initial states with current decoder output states\n",
    "        decoder_initial_states = [dec_h_state, dec_c_state]\n",
    "\n",
    "        # end condition\n",
    "        if english_predicted_int == hi_word_index['<end>'] or len(predicted_words) >= 20:\n",
    "            break\n",
    "\n",
    "    # F\n",
    "    # remove <end> from end\n",
    "    predicted_words = predicted_words[:-1]\n",
    "    return ' '.join([hi_index_word.get(ele, '') for ele in predicted_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "from nltk.translate import bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "bleu_score = []\n",
    "print(\"=\" * 50)     \n",
    "\n",
    "# sampling 1000 datapoints randomly from test set\n",
    "for index, (_, row) in enumerate(train.sample(1000).iterrows()):\n",
    "    input_sent = row.eng_sent\n",
    "    predicted_eng = predict(input_sent)\n",
    "    actual_eng = row.hi_out.replace('<end>','').strip()\n",
    "\n",
    "    # printing Translation Pairs\n",
    "    if (index + 1)%100 == 0:\n",
    "        print(f\"\\English sentence: {input_sent}\")\n",
    "        print(f\"Actual Translation: {actual_eng}\")\n",
    "        print(f\"Predicted Translation: {predicted_eng}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    bleu_score.append(sentence_bleu([actual_eng.split(),], predicted_eng.split()))\n",
    "\n",
    "print(f\"Mean Bleu Score = {np.mean(bleu_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        self.embedding = Embedding(inp_vocab_size, embedding_size, input_length=input_length)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.lstm_size = lstm_size\n",
    "        lstmcell = LSTMCell(lstm_size)\n",
    "        self.lstm = RNN(lstmcell, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, input_sequence, states):\n",
    "        '''\n",
    "            This function takes a sequence input and the initial states of the encoder.\n",
    "            Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "            returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "        '''\n",
    "        embed = self.embedding(input_sequence)\n",
    "        enc_out, enc_h, enc_c = self.lstm(embed, initial_state=states)\n",
    "        return enc_out, enc_h, enc_c\n",
    "\n",
    "    def initialize_states(self, batch_size):\n",
    "        '''\n",
    "            Given a batch size it will return intial hidden state and intial cell state.\n",
    "        '''\n",
    "        # we require tensor if we return numpy array it gives error\n",
    "        return tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "        Class that calculates similarity score based on the scoring_function using Bahdanu attention mechanism.\n",
    "    '''\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "        super().__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        if scoring_function == 'concat':\n",
    "            # Intialize variables needed for Concat score function here\n",
    "            # add dense layer for finding w1, w2 and V\n",
    "            self.tanh_activation = Activation('tanh')\n",
    "            self.dense_concat_1 = Dense(att_units)\n",
    "            self.dense_concat_2 = Dense(att_units)\n",
    "            self.dense_1 = Dense(1)\n",
    "        elif scoring_function == 'general':\n",
    "            # Intialize variables needed for General score function here\n",
    "            self.dense_general = Dense(att_units)      \n",
    "        \n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "        '''\n",
    "            Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "            * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "            Multiply the score function with your encoder_outputs to get the context vector.\n",
    "            Function returns context vector and attention weights(softmax - scores)\n",
    "        '''\n",
    "        if self.scoring_function == 'dot':\n",
    "            # Implement Dot score function here\n",
    "            # decoder_hidden_state = (16, 32)\n",
    "            decoder_hidden_state = tf.expand_dims(decoder_hidden_state, -1)\n",
    "            # decoder_hidden_state = (16, 32, 1)\n",
    "            # mul encoder = (16, 10, 32) and (16, 32, 1)\n",
    "            alpha = tf.matmul(encoder_output, decoder_hidden_state)\n",
    "            # we get alpha of shape (16, 10, 1)\n",
    "\n",
    "        elif self.scoring_function == 'concat':\n",
    "            # tanh((Hd(t-1) * W1 + He(t=n) * W2)) * V\n",
    "            # Implement General score function here\n",
    "            transformed_enc_out = self.dense_concat_1(encoder_output)\n",
    "            transformed_dec_hidden_state = self.dense_concat_2(decoder_hidden_state)\n",
    "\n",
    "            added_both = add([transformed_enc_out, tf.expand_dims(transformed_dec_hidden_state,1)])\n",
    "            added_both = self.tanh_activation(added_both)\n",
    "            alpha = self.dense_1(added_both)\n",
    "            # we get alpha of shape (16, 10, 1)\n",
    "\n",
    "        elif self.scoring_function == 'general':\n",
    "            # He(n,d) * W (d,d') * Hd(d',1)\n",
    "            # Implement General score function here\n",
    "            # pass encoder (16, 10, 32) to dense layer \n",
    "            transformed_enc_out = self.dense_general(encoder_output)\n",
    "            decoder_hidden_state = tf.expand_dims(decoder_hidden_state, -1)\n",
    "\n",
    "            alpha = tf.matmul(transformed_enc_out, decoder_hidden_state)\n",
    "            # we get alpha of shape (16, 10, 1)\n",
    "\n",
    "        # apply softmax on alphas\n",
    "        alpha = tf.squeeze(alpha, axis = -1)\n",
    "        attention_weights = Activation('softmax')(alpha)\n",
    "        # expand dimension of alpha to do matrix multiplication with encoder \n",
    "        attention_weights = tf.expand_dims(attention_weights, axis = -1)\n",
    "        \n",
    "        context_vector = tf.matmul(tf.transpose(encoder_output, perm = [0,2,1]), attention_weights)\n",
    "        # remove extra dimension\n",
    "        context_vector = tf.squeeze(context_vector, axis = -1)\n",
    "\n",
    "        return context_vector, attention_weights    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Class for finding translation word by word\n",
    "    '''\n",
    "    def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
    "        super().__init__()\n",
    "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.embedding = Embedding(tar_vocab_size, embedding_dim, input_length=input_length)\n",
    "        lstmcell = LSTMCell(dec_units)\n",
    "        self.lstm = RNN(lstmcell, return_sequences=False, return_state=True)\n",
    "        self.dense = Dense(tar_vocab_size)\n",
    "        self.attention = Attention(score_fun, att_units)\n",
    "\n",
    "    def call(self, input_to_decoder, encoder_output, state_h, state_c):\n",
    "        '''\n",
    "            One step decoder mechanisim step by step:\n",
    "        A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "        B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "        C. Concat the context vector with the step A output\n",
    "        D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "        E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "        F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        '''\n",
    "        # if this parameters then we get shapes of following\n",
    "        # tar_vocab_size=13 \n",
    "        # embedding_dim=12 \n",
    "        # input_length=10\n",
    "        # dec_units=16 \n",
    "        # att_units=16\n",
    "        # batch_size=32\n",
    "\n",
    "        # A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "        embeddings_input_dec = self.embedding(input_to_decoder)  #shape = (32, 1, 12)\n",
    "        \n",
    "        # B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "        context_vector, att_weights = self.attention(state_h, encoder_output)  # context_vector = (32, 16)\n",
    "\n",
    "        # C. Concat the context vector with the step A output\n",
    "        input_to_decoder = concatenate([embeddings_input_dec,tf.expand_dims(context_vector,1)])  #shape = (32, 1, 16)\n",
    "\n",
    "        # D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "        dec_out, dec_h_state, dec_c_state = self.lstm(input_to_decoder, initial_state=[state_h, state_c])\n",
    "\n",
    "        # E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "        predicted_out = self.dense(dec_out)\n",
    "\n",
    "        # output ,state_h ,state_c\n",
    "        # (32, 13) (32, 16) (32, 16)\n",
    "\n",
    "        # F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        return predicted_out, dec_h_state, dec_c_state, att_weights, context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_size, input_length, dec_units ,score_fun ,att_units):\n",
    "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        super().__init__()\n",
    "        self.onestepdecoder = One_Step_Decoder(out_vocab_size, embedding_size, input_length, dec_units, score_fun, att_units)\n",
    "\n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/TensorArray\n",
    "        all_outputs = tf.TensorArray(tf.float32, size = tf.shape(input_to_decoder)[1])\n",
    "\n",
    "        #Iterate till the length of the decoder input\n",
    "        for i in range(tf.shape(input_to_decoder)[1]):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, decoder_hidden_state, decoder_cell_state, attention_weights, context_vector = \\\n",
    "            self.onestepdecoder(input_to_decoder[:,i:i+1], encoder_output,decoder_hidden_state, decoder_cell_state)\n",
    "            # Store the all_outputs in tensorarray\n",
    "            all_outputs = all_outputs.write(i, output)\n",
    "        # Return the tensor \n",
    "        all_outputs = tf.transpose(all_outputs.stack(), perm = [1,0,2])\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_size, enc_lstm_units, dec_lstm_units, enc_input_length, dec_input_length, \\\n",
    "                 score_fun, att_units):\n",
    "        super().__init__()\n",
    "        #Intializing objects from encoder decoder\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_size, enc_lstm_units, enc_input_length)\n",
    "        self.decoder = Decoder(out_vocab_size, embedding_size, dec_input_length, dec_lstm_units, score_fun, att_units)\n",
    "\n",
    "    def call(self,data):\n",
    "        encoder_inputs = data[0]\n",
    "        decoder_inputs = data[1]\n",
    "        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "        encoder_initial_states = self.encoder.initialize_states(tf.shape(encoder_inputs)[0])\n",
    "        enc_out, enc_h_state, enc_c_state = self.encoder(encoder_inputs, encoder_initial_states)\n",
    "        # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "        # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "        dec_out = self.decoder(decoder_inputs, enc_out, enc_h_state, enc_c_state)\n",
    "\n",
    "        # return the decoder output\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Custom loss function that will not consider the loss for padded zeros.\n",
    "    why are we using this, can't we use simple sparse categorical crossentropy?\n",
    "    Yes, you can use simple sparse categorical crossentropy as loss like we did in task-1. But in this loss function we are ignoring the loss\n",
    "    for the padded zeros. i.e when the input is zero then we donot need to worry what the output is. This padded zeros are added from our end\n",
    "    during preprocessing to make equal length for all the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compiling the model\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "dot_model = encoder_decoder(inp_vocab_size = eng_vocab_size, out_vocab_size = hi_vocab_size,\n",
    "                            embedding_size = 128, enc_lstm_units = 128, dec_lstm_units = 128,\n",
    "                            enc_input_length = enc_input_length, dec_input_length = dec_input_length, \n",
    "                            score_fun = 'dot', att_units = 128)\n",
    "#compiling the model\n",
    "dot_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002), loss = loss_function)\n",
    "\n",
    "# callbacks\n",
    "# EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callback = [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "878/878 [==============================] - 492s 560ms/step - loss: 1.9380 - val_loss: 1.6529\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 496s 564ms/step - loss: 1.6174 - val_loss: 1.4990\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 1.4678 - val_loss: 1.4106\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 497s 566ms/step - loss: 1.3512 - val_loss: 1.3481\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 1.2525 - val_loss: 1.3043\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 497s 566ms/step - loss: 1.1659 - val_loss: 1.2714\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 497s 566ms/step - loss: 1.0897 - val_loss: 1.2462\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 497s 566ms/step - loss: 1.0212 - val_loss: 1.2315\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 497s 566ms/step - loss: 0.9606 - val_loss: 1.2235\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 0.9066 - val_loss: 1.2172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc77434a350>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 224\n",
    "# send data to data generators\n",
    "train_data_generator = data_generator(train_eng_inp_seq, train_dec_in_seq, train_dec_out_seq, batch_size)\n",
    "val_data_generator = data_generator(test_eng_inp_seq, test_dec_in_seq, test_dec_out_seq, batch_size)\n",
    "\n",
    "# model training\n",
    "dot_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
    "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
    "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "878/878 [==============================] - 494s 562ms/step - loss: 0.8588 - val_loss: 1.2183\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 497s 566ms/step - loss: 0.8165 - val_loss: 1.2215\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 0.7791 - val_loss: 1.2239\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 497s 566ms/step - loss: 0.7463 - val_loss: 1.2343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc73c6a0c10>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 224\n",
    "# send data to data generators\n",
    "# train_data_generator = data_generator(train_eng_inp_seq, train_dec_in_seq, train_dec_out_seq, batch_size)\n",
    "# val_data_generator = data_generator(test_eng_inp_seq, test_dec_in_seq, test_dec_out_seq, batch_size)\n",
    "\n",
    "# model training\n",
    "dot_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
    "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
    "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "878/878 [==============================] - 493s 561ms/step - loss: 1.9498 - val_loss: 1.6693\n",
      "Epoch 2/10\n",
      "878/878 [==============================] - 495s 563ms/step - loss: 1.6294 - val_loss: 1.5054\n",
      "Epoch 3/10\n",
      "878/878 [==============================] - 495s 564ms/step - loss: 1.4774 - val_loss: 1.4169\n",
      "Epoch 4/10\n",
      "878/878 [==============================] - 496s 564ms/step - loss: 1.3621 - val_loss: 1.3548\n",
      "Epoch 5/10\n",
      "878/878 [==============================] - 495s 564ms/step - loss: 1.2614 - val_loss: 1.3079\n",
      "Epoch 6/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 1.1726 - val_loss: 1.2742\n",
      "Epoch 7/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 1.0940 - val_loss: 1.2478\n",
      "Epoch 8/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 1.0233 - val_loss: 1.2326\n",
      "Epoch 9/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 0.9609 - val_loss: 1.2193\n",
      "Epoch 10/10\n",
      "878/878 [==============================] - 496s 565ms/step - loss: 0.9060 - val_loss: 1.2205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc73c675690>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 224\n",
    "# send data to data generators\n",
    "# train_data_generator = data_generator(train_eng_inp_seq, train_dec_in_seq, train_dec_out_seq, batch_size)\n",
    "# val_data_generator = data_generator(test_eng_inp_seq, test_dec_in_seq, test_dec_out_seq, batch_size)\n",
    "\n",
    "# model training\n",
    "dot_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
    "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
    "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer: https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\n",
    "def plot_attention(attention, encoder_inp, predicted):\n",
    "    heatmap_df = pd.DataFrame(attention, columns=encoder_inp, index=predicted)\n",
    "    plt.figure(figsize=(7, 10))\n",
    "    sns.heatmap(heatmap_df, cmap='YlGnBu', linewidths=.3)\n",
    "    plt.title(\"Attention Plot\")\n",
    "    plt.ylabel(\"English\")\n",
    "    plt.xlabel(\"Italian\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_attention(input_sentence, model, plot_attention_weights = False):\n",
    "    '''\n",
    "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    C. Initialize index of  as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "    D. till we reach max_length of decoder or till the model predicted word :\n",
    "            predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "            Save the attention weights\n",
    "            And get the word using the tokenizer(word index) and then store it in a string.\n",
    "    E. Call plot_attention(#params)\n",
    "    F. Return the predicted sentence\n",
    "    '''\n",
    "    # A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    # tokenization of the sentence \n",
    "    tokenized_encoder_input = token_eng.texts_to_sequences([input_sentence])\n",
    "    # padding the sequence\n",
    "    encoder_input = pad_sequences(tokenized_encoder_input, maxlen=max_len, padding='post')\n",
    "\n",
    "    # B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    enc_init_states = model.layers[0].initialize_states(1)\n",
    "    enc_out, enc_h_state, enc_c_state = model.layers[0](encoder_input, states = enc_init_states)\n",
    "    # initializing decoder states\n",
    "    decoder_h_state = enc_h_state\n",
    "    decoder_c_state = enc_c_state\n",
    "\n",
    "    # C. Initialize index of  as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "    # decoder initial input\n",
    "    decoder_initial_input = np.zeros((1,1))\n",
    "    decoder_initial_input[0,0] = hi_word_index['<start>']\n",
    "\n",
    "    # D. till we reach max_length of decoder or till the model predicted word :\n",
    "    predicted_words = []\n",
    "    att_weights_all = []\n",
    "    predicting = True\n",
    "    while predicting:\n",
    "        # predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "        prediction, decoder_h_state, decoder_c_state, att_weights,_ = model.layers[1].onestepdecoder(decoder_initial_input, \n",
    "                                                                                                                  enc_out, \n",
    "                                                                                                                  decoder_h_state,\n",
    "                                                                                                                  decoder_c_state)\n",
    "        #predicted english token\n",
    "        english_predicted_int = np.argmax(prediction.numpy().ravel())\n",
    "        predicted_words.append(english_predicted_int)\n",
    "        \n",
    "        # Save the attention weights\n",
    "        att_weights_all.append(att_weights.numpy().ravel())\n",
    "        decoder_initial_input[0,0] = english_predicted_int\n",
    "        # break condition\n",
    "        if english_predicted_int == hi_word_index['<end>'] or len(predicted_words)>=20:\n",
    "            break\n",
    "    \n",
    "    #checking for non-padding words in encoder input\n",
    "    att_weights_all = np.array(att_weights_all)\n",
    "    non_padded_encoder_input = np.where(encoder_input[0] != 0)[0]\n",
    "    encoder_input_words = np.array(input_sentence.split())[non_padded_encoder_input]\n",
    "    # get the word using the tokenizer(word index) and then store it in a string.\n",
    "    decoder_output_words = [hi_index_word.get(ele, '') for ele in predicted_words]\n",
    "    #keeping only those attention weights corresponding to non-padded words\n",
    "    att_weights_all = att_weights_all[:,non_padded_encoder_input]\n",
    "\n",
    "    # E. Call plot_attention(#params)\n",
    "    if plot_attention_weights:\n",
    "        plot_attention(att_weights_all, encoder_input_words, decoder_output_words)\n",
    "    else:\n",
    "        # F. Return the predicted sentence\n",
    "        return ' '.join(decoder_output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "bleu_score = []\n",
    "print(\"=\" * 50)     \n",
    "\n",
    "# sampling 1000 datapoints randomly from test set\n",
    "for index, (_, row) in enumerate(train.sample(1000).iterrows()):\n",
    "    input_sent = row.eng_sent\n",
    "    predicted_eng = predict_attention(input_sent, dot_model, plot_attention_weights = False)\n",
    "    actual_eng = row.hi_out.replace('','').strip()\n",
    "    predicted_eng = predicted_eng.replace('','').strip()\n",
    "\n",
    "    # printing Translation Pairs\n",
    "    if (index + 1)%100 == 0:\n",
    "        print(f\"\\English sentence: {input_sent}\")\n",
    "        print(f\"Actual Translation: {actual_eng}\")\n",
    "        print(f\"Predicted Translation: {predicted_eng}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    bleu_score.append(sentence_bleu([actual_eng.split(),], predicted_eng.split()))\n",
    "\n",
    "print(f\"Mean Bleu Score = {np.mean(bleu_score)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_2.2.0",
   "language": "python",
   "name": "tensorflow_2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
